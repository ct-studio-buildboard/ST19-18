{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Segmentation Algorithm\n",
    "1. [Project Highlight](#Project-Highlight)\n",
    "2. [Introduction](#Introduction)\n",
    "3. [AWS Authentication](#AWS-Authentication)\n",
    "4. [Data Preparation](#Data-Preparation)\n",
    "  1. [Download data](#Download-data)\n",
    "  2. [Setup Data](#Setup-data)\n",
    "  3. [Upload to S3](#Upload-to-S3)\n",
    "5. [Training](#Training)\n",
    "6. [Hosting](#Hosting)\n",
    "7. [Delete The Endpoint](#Delete-the-Endpoint)\n",
    "\n",
    "## Project Highlight\n",
    "\n",
    "In an effort to prototype our startup studio project, we used the camera attached to our small RC car model which is hosted on a Raspberry Pi. The camera gives us a better understanding of the obstaclese surrounding the vehicle and is streamed to AWS using Amazon Kinesis Video Streams. Once input from the stream is received we are able to then use AWS sagemarker to analyze the stream and send the output to a S3 bucket. From there a AWS Lambda function reads from the S3 bucket and kickstarts the decoding process.\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Semantic Segmentation is the task of classifying every pixel in an image with a class from a known set of labels. The output is an integer matrix of the same shape as the input frame. Scene parsing provides complete understanding of the scene. Within the context of this project, this is critical. The mask offers us a highly condensed version of the frame with the same semantic information still encoded.\n",
    "\n",
    "The purpose of this notebook is to create the section of our architectural design: a Pyramid Scene Parsing Network that converts the given frame to a segmented mask. More information about the Pyramid Scene Parsing Network can be found within this paper([PSP](https://arxiv.org/abs/1612.01105)). Additionally, the parameters used are the same as those mentioned in the paper previously cited.\n",
    "\n",
    "\n",
    "## AWS Authentication\n",
    "Authenticate the use of AWS services for the startup studio IAM role we have set up for this project. Permissions: access to SageMaker, access to a advanced computing machine, and access to all S3 buckets associated with the account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    " \n",
    "role = get_execution_role()\n",
    "print(role)\n",
    "sess = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creates a S3 bucket storing training data and all the artifacts created by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = sess.default_bucket()  \n",
    "prefix = 'xporter-segmentation'\n",
    "print(bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we need the Amazon SageMaker Semantic Segmentaion docker image, which is static and need not be changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "training_image = get_image_uri(sess.boto_region_name, 'semantic-segmentation', repo_version=\"latest\")\n",
    "print (training_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "[Pascal VOC](http://host.robots.ox.ac.uk/pascal/VOC/) is a popular computer vision dataset which is used for annual semantic segmentation challenges from 2005 to 2012. The dataset has 1464 training and 1449 validation images with 21 classes. Examples of the segmentation dataset can be seen in the [Pascal VOC Dataset page](http://host.robots.ox.ac.uk/pascal/VOC/voc2012/segexamples/index.html). The classes are as follows:\n",
    "\n",
    "| Label Id |     Class     |\n",
    "|:--------:|:-------------:|\n",
    "|     0    |   Background  |\n",
    "|     1    |   Aeroplane   |\n",
    "|     2    |    Bicycle    |\n",
    "|     3    |      Bird     |\n",
    "|     4    |      Boat     |\n",
    "|    5     |     Bottle    |\n",
    "|     6    |      Bus      |\n",
    "|     7    |      Car      |\n",
    "|     8    |      Cat      |\n",
    "|     9    |     Chair     |\n",
    "|    10    |      Cow      |\n",
    "|    11    |  Dining Table |\n",
    "|    12    |      Dog      |\n",
    "|    13    |     Horse     |\n",
    "|    14    |   Motorbike   |\n",
    "|    15    |     Person    |\n",
    "|    16    |  Potted Plant |\n",
    "|    17    |     Sheep     |\n",
    "|    18    |      Sofa     |\n",
    "|    19    |     Train     |\n",
    "|    20    |  TV / Monitor |\n",
    "|    255   | Hole / Ignore |\n",
    "\n",
    "In this notebook, we will use the data sets from 2012. While using the Pascal VOC dataset, please be aware of the  usage rights:\n",
    "\"The VOC data includes images obtained from the \"flickr\" website. Use of these images must respect the corresponding terms of use: \n",
    "* \"flickr\" terms of use (https://www.flickr.com/help/terms)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download data\n",
    "Download the Pascal VOC datasets from VOC 2012. This section only needs to be run once, after the first time we just grab everything from the S3 bucket we made. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Download the dataset\n",
    "!wget -P /tmp http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar    \n",
    "# # Extract the data.\n",
    "!tar -xf /tmp/VOCtrainval_11-May-2012.tar && rm /tmp/VOCtrainval_11-May-2012.tar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup data\n",
    "Move the downloaded data into S3 buckets with the correct directories so that it is easy to sort. This pattern is the same as the recommended structure from PASCAL. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Create directory structure mimicing the s3 bucket where data is to be dumped.\n",
    "VOC2012 = 'VOCdevkit/VOC2012'\n",
    "os.makedirs('train', exist_ok=True)\n",
    "os.makedirs('validation', exist_ok=True)\n",
    "os.makedirs('train_annotation', exist_ok=True)\n",
    "os.makedirs('validation_annotation', exist_ok=True)\n",
    "\n",
    "# Create a list of all training images.\n",
    "filename = VOC2012+'/ImageSets/Segmentation/train.txt'\n",
    "with open(filename) as f:\n",
    "    train_list = f.read().splitlines() \n",
    "\n",
    "# Create a list of all validation images.\n",
    "filename = VOC2012+'/ImageSets/Segmentation/val.txt'\n",
    "with open(filename) as f:\n",
    "    val_list = f.read().splitlines() \n",
    "\n",
    "# Move the jpg images in training list to train directory and png images to train_annotation directory.\n",
    "for i in train_list:\n",
    "    shutil.copy2(VOC2012+'/JPEGImages/'+i+'.jpg', 'train/')\n",
    "    shutil.copy2(VOC2012+'/SegmentationClass/'+i+'.png','train_annotation/' )\n",
    "\n",
    "# Move the jpg images in validation list to validation directory and png images to validation_annotation directory.\n",
    "for i in val_list:\n",
    "    shutil.copy2(VOC2012+'/JPEGImages/'+i+'.jpg', 'validation/')\n",
    "    shutil.copy2(VOC2012+'/SegmentationClass/'+i+'.png','validation_annotation/' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The current structure of the S3 bucket should look like this:\n",
    "\n",
    "```bash\n",
    "root \n",
    "|-train/\n",
    "|-train_annotation/\n",
    "|-validation/\n",
    "|-validation_annotation/\n",
    "\n",
    "```\n",
    "\n",
    "We will let AWS know that we want the image names (which are integers) in \"annotation\" directories to be read as labels directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "label_map = { \"scale\": 1 }\n",
    "with open('train_label_map.json', 'w') as lm_fname:\n",
    "    json.dump(label_map, lm_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create channel names for the s3 bucket.\n",
    "train_channel = prefix + '/train'\n",
    "validation_channel = prefix + '/validation'\n",
    "train_annotation_channel = prefix + '/train_annotation'\n",
    "validation_annotation_channel = prefix + '/validation_annotation'\n",
    "# label_map_channel = prefix + '/label_map'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload to S3\n",
    "Let's upload the data with the label map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# upload the appropraite directory up to s3 respectively for all directories.\n",
    "sess.upload_data(path='train', bucket=bucket, key_prefix=train_channel)\n",
    "sess.upload_data(path='validation', bucket=bucket, key_prefix=validation_channel)\n",
    "sess.upload_data(path='train_annotation', bucket=bucket, key_prefix=train_annotation_channel)\n",
    "sess.upload_data(path='validation_annotation', bucket=bucket, key_prefix=validation_annotation_channel)\n",
    "# sess.upload_data(path='train_label_map.json', bucket=bucket, key_prefix=label_map_channel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to setup an output location at S3, where the model artifact will be dumped. These artifacts are also the output of the algorithm's traning job. Let us use another channel in the same S3 bucket for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_output_location = 's3://{}/{}/output'.format(bucket, prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "With the training data all complete, it is time to actually train the model. We will be using a advanced computer (ml.p2.xlarge) to speed up the training time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the sagemaker estimator object.\n",
    "ss_model = sagemaker.estimator.Estimator(training_image,\n",
    "                                         role, \n",
    "                                         train_instance_count = 1, \n",
    "                                         train_instance_type = 'ml.p2.xlarge',\n",
    "                                         train_volume_size = 50,\n",
    "                                         train_max_run = 360000,\n",
    "                                         output_path = s3_output_location,\n",
    "                                         base_job_name = 'xporter-segmentation',\n",
    "                                         sagemaker_session = sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the paper the following the PSPNet(50) got strong results. The mean IoU was 41.68% and the ovrall pixel accuracy was 80.04%. As noted in the paper preformance grows with deeper networks, but we are going with just resnet 50 because speed is critical. The faster we make the segmentation process, the faster the remote driver is going to be able to make a decision about how to operate the vehicle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setup hyperparameters \n",
    "ss_model.set_hyperparameters(backbone='resnet-50', \n",
    "                             algorithm='psp', # PSPNet(50)                              \n",
    "                             use_pretrained_model='True', # Use the pre-trained model.\n",
    "                             crop_size=240,                              \n",
    "                             num_classes=21, # Pascal has 21 classes, see the data prep for the list of classes.\n",
    "                             epochs=10,\n",
    "                             learning_rate=0.0001,                             \n",
    "                             optimizer='rmsprop', \n",
    "                             lr_scheduler='poly',                          \n",
    "                             mini_batch_size=16, \n",
    "                             validation_mini_batch_size=16,\n",
    "                             early_stopping=True, \n",
    "                             early_stopping_patience=2, # Tolerate these many epochs if the mIoU doens't increase.\n",
    "                             early_stopping_min_epochs=10, # No matter what, run these many number of epochs.                             \n",
    "                             num_training_samples=num_training_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the hyperparameters are setup, let us prepare the handshake between our data channels and the algorithm. To do this, we need to create the `sagemaker.session.s3_input` objects from our data channels. These objects are then put in a simple dictionary, which the algorithm uses to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "distribution = 'FullyReplicated'\n",
    "# Create sagemaker s3_input objects\n",
    "train_data = sagemaker.session.s3_input('s3://{}/{}'.format(bucket, train_channel), \n",
    "                                        distribution=distribution, \n",
    "                                        content_type='image/jpeg', \n",
    "                                        s3_data_type='S3Prefix')\n",
    "\n",
    "validation_data = sagemaker.session.s3_input('s3://{}/{}'.format(bucket, validation_channel),\n",
    "                                             distribution=distribution, \n",
    "                                             content_type='image/jpeg', \n",
    "                                             s3_data_type='S3Prefix')\n",
    "\n",
    "train_annotation = sagemaker.session.s3_input('s3://{}/{}'.format(bucket, train_annotation_channel), \n",
    "                                              distribution=distribution, \n",
    "                                              content_type='image/png', \n",
    "                                              s3_data_type='S3Prefix')\n",
    "\n",
    "validation_annotation = sagemaker.session.s3_input('s3://{}/{}'.format(bucket, validation_annotation_channel), \n",
    "                                              distribution=distribution, \n",
    "                                              content_type='image/png', \n",
    "                                              s3_data_type='S3Prefix')\n",
    "data_channels = {'train': train_data, \n",
    "                 'validation': validation_data,\n",
    "                 'train_annotation': train_annotation, \n",
    "                 'validation_annotation':validation_annotation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we want to train the model and store the model in our S3 bucket This is important because it makes it possible for this model to be used in future situations. The future situations we have in mind are when each frame comes in via the Kinesis stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_model.fit(inputs=data_channels, logs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hosting\n",
    "Once the training is done, we can deploy the trained model as an Amazon SageMaker hosted endpoint. This will allow us to make predictions (or inference) from the model. Note that we don't have to host on the same instance (or type of instance) that we used to train. \n",
    "\n",
    "Once the endpoint is up we can test individual images by writing the following:\n",
    "\n",
    "```\n",
    "\n",
    "ss_predictor.content_type = 'image/jpeg'\n",
    "ss_predictor.accept = 'image/png'\n",
    "return_img = ss_predictor.predict(img)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ss_predictor = ss_model.deploy(initial_instance_count=1, instance_type='ml.t2.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete the Endpoint\n",
    "Noting is free in the world. So turn the endpoint off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sagemaker.Session().delete_endpoint(ss_predictor.endpoint)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
